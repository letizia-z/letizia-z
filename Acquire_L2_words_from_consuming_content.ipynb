{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/letizia-z/letizia-z/blob/main/Acquire_L2_words_from_consuming_content.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq5AKSgvjn5N"
      },
      "source": [
        "|     Course                     | Academic Year |\n",
        "|    :---                        |     ---:      |\n",
        "| Programming for the Humanities |  *2023/2024*  |\n",
        "\n",
        "*This has been my first programming course.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRwnvgxcjn5Q"
      },
      "source": [
        "# HOW MANY WORDS CAN I ACQUIRE FROM CONSUMING CONTENT IN L2\n",
        "\n",
        "### Project Description\n",
        "\n",
        "This project uses Python to perform linguistic analysis on three short stories. The goal is to extract useful information regarding the complexity and variety of the language used in the texts, as well as the number of occurrences of each word. This tool is particularly useful for those who want to deepen their linguistic understanding and improve their vocabulary through the consumption of content in the target language.\n",
        "\n",
        "* **Input data:** three short stories in English\n",
        "* **Output data:** sentence, word, and syllable count; Flesch-Kincaid score and reading difficulty; vocabulary variety; possible encounter with new words and passively learnable words (i.e., those that exceed a certain number of occurrences within the texts).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmQ6f8RKjn5Q"
      },
      "source": [
        "### Name and URL of programs/notebooks reused in the project\n",
        "\n",
        "|Name|URL|\n",
        "| :---        |    :---  |\n",
        "|*Python*|*Notebooks from lectures, especially \"09_NLP\"*|\n",
        "||*https://www.datacamp.com/tutorial/sort-a-dictionary-by-value-python*|\n",
        "|*Spacy*|*https://spacy.io/api/doc*|\n",
        "||https://stackoverflow.com/questions/405161/detecting-syllables-in-a-word|\n",
        "|*Matplotlib.pyplot* |*https://matplotlib.org/stable/api/pyplot_summary.html*|\n",
        "||*https://stackoverflow.com/questions/66446687/how-do-i-make-a-dashed-horizontal-line-with-matplotlib* |\n",
        "|*Artificial Intelligence*|*ChatGPT, Gemini*|\n",
        "\n",
        "*This code was developed with the assistance of artificial intelligence (AI) tools to generate a starting point or suggestions. However, the final code has been reviewed, modified, and adapted according to my specific needs, and represents the result of personal work. Any similarities with other works are purely coincidental and unintentional. I have taken all necessary precautions to ensure that the code presented here does not constitute plagiarism and respects copyright laws.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YClMhsuVjn5Q"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# 1. INTRODUCTION\n",
        "\n",
        "Anyone who has seriously studied a foreign language has likely experienced the so-called ***“language learning plateau”*** — that moment when you already have enough vocabulary to understand and express more or less everything you want, making it increasingly difficult to learn new words.\n",
        "To overcome this plateau, reading or generally increasing your consumption of content in the target language is often recommended.\n",
        "With this in mind, I decided to analyze some texts myself to see **how effective content consumption really is**, in this case with reference to the **English language**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FplgHsjjn5R"
      },
      "source": [
        "## 1.1 SOME BASIC CONCEPTS\n",
        "\n",
        "Before diving into the actual project, it's important to provide two key pieces of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2IkZerOjn5R"
      },
      "source": [
        "### 1.1.1 CEFR LEVELS\n",
        "Each language proficiency level includes an approximate number of words that are known and usable by the speaker. Therefore, depending on one's starting level, there will be differences in the time needed to understand a text and in the number of words that can be learned from it.\n",
        "\n",
        "| LEVEL | WORDS | HOURS |\n",
        "| --- | --- | --- |\n",
        "| A1 | 700 | 100 |\n",
        "| A2 | 1500 | 180/200 |\n",
        "| B1 | 2500 | 350/400 |\n",
        "| B2 | 4000 | 500/600 |\n",
        "| C1 | 8000 | 700/800 |\n",
        "| C2 | 16000 | 1000/1200 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs3mA5Vzjn5R"
      },
      "source": [
        "### 1.1.2 VOCABULARY ACQUISITION\n",
        "\n",
        "Simply encountering a word in a text is obviously not enough to learn it. When it comes to learning a new word, we can take two main approaches:\n",
        "\n",
        "* **Active learning**, where the student who comes across a new term makes a conscious effort to remember it (for example, by using flashcards)\n",
        "* **Passive acquisition**, which relies primarily on repeated exposure to the same word, ideally in different contexts\n",
        "\n",
        "Most studies in this field focus on first language (L1) acquisition rather than second language (L2) learning. This is partly because it is still unclear how many exposures are needed to acquire a word, as this also depends on individual cognitive abilities.\n",
        "\n",
        "According to Uchihara et al.:\n",
        "\n",
        "> *“the number of encounters necessary to learn words rang\\[es] from 6, 10, 12, to more than 20 times. \\[That is to say,] the number of encounters necessary for learning of vocabulary to occur during meaning-focussed input remains unclear”*\n",
        "\n",
        "Therefore, for the purposes of my project, I decided to assume that the **minimum number of exposures required for passive vocabulary acquisition is 12**, based in part on a study by Holly L. Storkel et al. on L1 acquisition in children.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B0iI-gQjn5R"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# 2. THE SHORT STORIES\n",
        "\n",
        "First, I selected three short stories that I was unfamiliar with, written by authors from different time periods, genders, and styles. The idea behind this choice was that **greater variety** would allow for the encounter of the largest possible number of different words. This is ideal from the perspective of *active vocabulary study*, but it could be problematic for *passive acquisition*, since a wider vocabulary range would likely result in fewer words reaching the 12-occurrence threshold.\n",
        "\n",
        "The short stories analyzed are:\n",
        "\n",
        "* *“The Yellow Wallpaper”* by C. P. Gilman (1892)\n",
        "* *“Hills Like White Elephants”* by E. Hemingway (1927)\n",
        "* *“A Good Man is Hard to Find”* by F. O’Connor (1953)\n",
        "\n",
        "\n",
        "\n",
        "> Note: make sure to manually download them in your personal Colab space and runtime\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzQTIMBrjn5S"
      },
      "source": [
        "## 2.1 IMPORTING AND OPENING THE FILES\n",
        "\n",
        "First, we'll need to open the files of the selected short stories, so we can begin analyzing them. To make sure I’ve opened the correct files, I’ll also print the first 100 characters of each one.\n",
        "\n",
        "To distinguish between the three texts, we’ll add the initial of each author’s last name to the variable names:\n",
        "\n",
        "* **O** = *“A Good Man is Hard to Find”* by F. O’Connor (1953)\n",
        "* **H** = *“Hills Like White Elephants”* by E. Hemingway (1927)\n",
        "* **G** = *“The Yellow Wallpaper”* by C. P. Gilman (1892)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVM2oE5ejn5S",
        "outputId": "fc8eeaf8-edc7-479b-a282-adc50ea81465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A GOOD MAN IS HARD TO FIND\n",
            "Flannery O’Connor, 1953\n",
            "The grandmother didn’t want to go to Florida. Sh\n",
            "\n",
            "﻿HILLS LIKE WHITE ELEPHANTS\n",
            "Ernest Hemingway, 1927 \n",
            "The hills across the valley of the Ebro were lon\n",
            "\n",
            "﻿THE YELLOW WALLPAPER\n",
            "Charlotte Perkins Gillman, 1892\n",
            "It is very seldom that mere ordinary people li\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def readFile(filePath):\n",
        "  with open(filePath, 'r', encoding='utf-8') as file:\n",
        "    return file.read()\n",
        "\n",
        "\n",
        "filePathO = 'short_stories/AGoodManIsHardToFind_OConnor1953.txt'\n",
        "filePathH = 'short_stories/HillsLikeWhiteElephants_Hemingway1927.txt'\n",
        "filePathG = 'short_stories/TheYellowWallpaper_Gillman1892.txt'\n",
        "\n",
        "rawTextO = readFile(filePathO)\n",
        "rawTextH = readFile(filePathH)\n",
        "rawTextG = readFile(filePathG)\n",
        "\n",
        "print(str(rawTextO[:100]) + '\\n')\n",
        "print(str(rawTextH[:100]) + '\\n')\n",
        "print(str(rawTextG[:100]) + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVUQzQyFjn5T"
      },
      "source": [
        "### 2.1.1 EXTRACTING THE TITLE\n",
        "\n",
        "I also decided to take advantage of the formatting of these files (with the title written in uppercase) to create a function that extracts only the title of the short story. This way, we can easily refer back to it in later stages, especially when displaying the results of the various analysis steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR_5rhHejn5T",
        "outputId": "96ec936a-79ac-4f4e-ddf0-053ac7737a17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A GOOD MAN IS HARD TO FIND\n",
            "\n",
            "﻿HILLS LIKE WHITE ELEPHANTS\n",
            "\n",
            "﻿THE YELLOW WALLPAPER\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def extractTitle(filePath):\n",
        "  with open(filePath, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "      strippedLine = line.strip() # remove blank spaces at the beginning and at the end of the line\n",
        "      if strippedLine.isupper(): # the title is supposedly in uppercase\n",
        "        return strippedLine\n",
        "  return 'Title not found'  # in cas the title isn't in uppercase like expected\n",
        "\n",
        "titleO = extractTitle(filePathO)\n",
        "print(str(titleO) + '\\n')\n",
        "\n",
        "titleH = extractTitle(filePathH)\n",
        "print(str(titleH) + '\\n')\n",
        "\n",
        "titleG = extractTitle(filePathG)\n",
        "print(str(titleG) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKHv2ZJPjn5T"
      },
      "source": [
        "## 2.2 PREPROCESSING\n",
        "\n",
        "One last necessary step is preprocessing the text by making slight modifications to simplify the subsequent analysis:\n",
        "\n",
        "* Convert the entire text to **lowercase**: this ensures that during co-occurrence counting, identical words are counted together (1), rather than being treated as separate groups due to capitalization\n",
        "* Remove **apostrophes**: I encountered issues related to apostrophes during tokenization, so I decided to remove them immediately, verifying that this neither affected tokenization nor influenced the later counts (2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOvpUKKijn5T",
        "outputId": "0bd81264-4a78-45a2-d591-478a296caec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a good man is hard to find\n",
            "flannery oconnor, 1953\n",
            "the grandmother didnt want to go to florida. she \n",
            "\n",
            "﻿hills like white elephants\n",
            "ernest hemingway, 1927 \n",
            "the hills across the valley of the ebro were lon\n",
            "\n",
            "﻿the yellow wallpaper\n",
            "charlotte perkins gillman, 1892\n",
            "it is very seldom that mere ordinary people li\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower() #(1)\n",
        "  text = text.replace('’', '') #(2)\n",
        "  return text\n",
        "\n",
        "textO = preprocess(rawTextO)\n",
        "textH = preprocess(rawTextH)\n",
        "textG = preprocess(rawTextG)\n",
        "\n",
        "print(str(textO[:100]) + '\\n')\n",
        "print(str(textH[:100]) + '\\n')\n",
        "print(str(textG[:100]) + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeZWqiqmjn5T"
      },
      "source": [
        "\n",
        "---\n",
        "# 3. FLESCH-KINCAID READABILITY\n",
        "\n",
        "The first thing we want to do is determine which text would be best to read first, moving from the easiest to the most difficult in order to **gradually build our vocabulary**.\n",
        "To do this, for English we can use the Flesch-Kincaid Grade Level Formula:\n",
        "\n",
        "$$\n",
        "0.39 \\cdot \\frac{\\text{total words}}{\\text{total sentences}} + 11.8 \\cdot \\frac{\\text{total syllables}}{\\text{total words}} - 15.59\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bXWII-8jn5U"
      },
      "source": [
        "## 3.1 CALCULATING THE VALUES\n",
        "\n",
        "From the formula, we see that we need to calculate three values:\n",
        "\n",
        "* Total sentences (`totalSentences`)\n",
        "* Total words (`totalWords`)\n",
        "* Total syllables (`totalSyllables`)\n",
        "\n",
        "To do this, we’ll use the `spaCy` library, downloading its English language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKNpQBOGjn5U",
        "outputId": "83c3662f-967a-41fc-d21e-b2d89773b94d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.8.7-cp313-cp313-macosx_11_0_arm64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.13-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.10-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Using cached typer-0.16.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from spacy) (2.32.4)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Requirement already satisfied: jinja2 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from spacy) (25.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Using cached cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Using cached smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from jinja2->spacy) (3.0.2)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/letiziazanetti/Library/Python/3.13/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading spacy-3.8.7-cp313-cp313-macosx_11_0_arm64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp313-cp313-macosx_11_0_arm64.whl (41 kB)\n",
            "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.13-cp313-cp313-macosx_11_0_arm64.whl (26 kB)\n",
            "Downloading numpy-2.3.2-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading preshed-3.0.10-cp313-cp313-macosx_11_0_arm64.whl (124 kB)\n",
            "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Downloading pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp313-cp313-macosx_11_0_arm64.whl (632 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.8/632.8 kB\u001b[0m \u001b[31m410.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.6-cp313-cp313-macosx_11_0_arm64.whl (832 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.7/832.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached typer-0.16.0-py3-none-any.whl (46 kB)\n",
            "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-1.3.0-cp313-cp313-macosx_11_0_arm64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Using cached cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
            "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "Using cached rich-14.1.0-py3-none-any.whl (243 kB)\n",
            "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Using cached smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
            "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading marisa_trie-1.2.1-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
            "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl (38 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, typing-inspection, tqdm, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, marisa-trie, cloudpathlib, click, catalogue, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
            "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 click-8.2.1 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 numpy-2.3.2 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 rich-14.1.0 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 tqdm-4.67.1 typer-0.16.0 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall spacy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43men_core_web_sm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/spacy/__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/spacy/util.py:484\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
            "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbazBKzHjn5U"
      },
      "source": [
        "## 3.1.1 TOTAL WORDS\n",
        "While drafting the project, I decided to start with word tokenization, so that I could immediately spot any potential errors that might also affect later stages of the analysis. In fact, this turned out to be one of the steps where I encountered the most challenges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc2_pecyjn5U"
      },
      "source": [
        "Knowing that I would eventually need proper word tokenization for later steps, I decided to create a function dedicated solely to that task. I then obtained the total number of tokens simply by printing the result with a `len()` call outside the function.\n",
        "\n",
        "However, during this first tokenization attempt, I noticed two main issues:\n",
        "\n",
        "1. A `\\ufeff` character (BOM – Byte Order Mark) appeared at the beginning of the text\n",
        "2. Punctuation and line breaks were being counted as tokens, even though I only wanted to include **words and numbers**\n",
        "\n",
        "<div style=\"text-align: center\">\n",
        "<img src=pics/token_ufeff.png width=75%/>\n",
        "</div>\n",
        "\n",
        "To address these issues:\n",
        "\n",
        "* I started tokenization from the first word after any potential BOM\n",
        "\n",
        "  * Using `.remove` would not be suitable, as it would also remove the first word after the BOM — in this case, the word \"A\" (3)\n",
        "* I defined the function so that it would only add to the token list those strings that consist entirely of letters (4)\n",
        "\n",
        "> *Note:* the use of `token.is_alpha` filters out all tokens containing apostrophes — including the author's name (\"O’Connor\"). This problem was already resolved during the **preprocessing phase** (see 2.2.1).\n",
        "\n",
        "The final function is therefore as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKUaKOTYjn5U"
      },
      "outputs": [],
      "source": [
        "def tokenizeWords(text):\n",
        "  if text.startswith('\\ufeff'): #(3)\n",
        "    text = text[1:]\n",
        "\n",
        "  doc = nlp(text)\n",
        "  words = []\n",
        "  for token in doc:\n",
        "    if token.is_alpha: #(4)\n",
        "      words.append(token.text)\n",
        "  return words\n",
        "\n",
        "tokensO = tokenizeWords(textO)\n",
        "totalWordsO = len(tokensO)\n",
        "print(tokensO)\n",
        "print('Total words: ' + str(totalWordsO)+ '\\n')\n",
        "\n",
        "tokensH = tokenizeWords(textH)\n",
        "totalWordsH = len(tokensH)\n",
        "print(tokensH)\n",
        "print('Total words: ' + str(totalWordsH) + '\\n')\n",
        "\n",
        "tokensG = tokenizeWords(textG)\n",
        "totalWordsG = len(tokensG)\n",
        "print(tokensG)\n",
        "print('Total words: ' + str(totalWordsG)+ '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vY_ecxqjn5U"
      },
      "source": [
        "This way, we also see that all contractions reappear (e.g., `'not'` becomes `'nt'`), as they are still recognized as individual tokens despite the absence of the apostrophe.\n",
        "\n",
        "In this regard, the only two letters that could pose issues are **'d'** (from *would*, *had*) and especially **'s'**. After checking, I observed the following:\n",
        "\n",
        "* **'d'** is always treated as a separate token\n",
        "* **'s'** is treated as a separate token **only** when it follows *wh-* or *th-* words. In contrast, in words like *its*, *lets*, or proper nouns, it’s interpreted as a plural, third person singular verb, or pronoun — and thus **merged with the preceding word**\n",
        "\n",
        "  * Regarding this, I figured that distinguishing the **Saxon genitive** from a plural word wasn’t particularly necessary for the purpose of estimating **reading difficulty**, since it’s one of the first things learners pick up and doesn't have a meaningful standalone form\n",
        "  * The same applies to **verbs** — especially since in the lemmatization step (*see 4.1 Lemmatization*) we already know that the verb *to be* will appear countless times (thus enough to be considered), and plurals will be lemmatized to their singular form regardless\n",
        "\n",
        "In short, I decided these distinctions weren’t relevant enough to justify more complex filtering at this stage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muSMxQ0jjn5U"
      },
      "source": [
        "## 3.1.2 TOTAL SYLLABLES\n",
        "To count the syllables, I used an additional spaCy pipeline called `spacy_syllables`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdhC9c6njn5U"
      },
      "outputs": [],
      "source": [
        "!pip install spacy spacy_syllables\n",
        "\n",
        "import spacy_syllables\n",
        "nlp.add_pipe('syllables')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jm2ZWs02jn5U"
      },
      "source": [
        "\n",
        "This function relies on the `._.syllable_count` method (5) to compute the number of syllables."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.13.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}